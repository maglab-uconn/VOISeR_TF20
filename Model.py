import tensorflow as tf
import numpy as np
from threading import Thread
import time, os, sys, json, argparse, pickle
from collections import Counter

from ProgressBar import progress

from Feeder import Feeder
from Feeder import Load_Data as Load_Variable
import Modules
import Analyzer

with open('Hyper_Parameters.json', 'r') as f:
    hp_Dict = json.load(f)

variable_Dict = Load_Variable()

class VOISeR:
    def __init__(self, start_Epoch, max_Epoch, export_Path):    # This function run at initializing timing.    
        self.feeder = Feeder(start_Epoch, max_Epoch)    #Feeder is pattern generator. Model get training and inference pattern from feeder. Please see the 'Feeder.py'
        self.export_Path = export_Path  #All inference and checkpoint are saved at export path. You can set export path at 'Hyper_Parameter.json'

        self.Model_Generate()   # Model is generated by this line. To know details, please see 'Model_Generate' function.

    def Model_Generate(self):   # This function is called at __init__(). Thus, it run automatically at initializing timing.
        self.layer_Dict = {}
        self.layer_Dict['Input'] = tf.keras.layers.Input(shape=[self.feeder.orthography_Size], dtype= tf.int32) #It define input structure to '[?, orthographic size]'. Orthographic size is the maximum word length.
        self.layer_Dict['Embedding'] = Modules.Embedding(input_dim= len(self.feeder.letter_List))(self.layer_Dict['Input'], self.feeder.max_Pronunciation_Length)   #Basically, orthographic input is integer numbers. This change them to a vector. Please see the 'Modules.py'
        self.layer_Dict['RNN'] = Modules.RNN(projection_Size= self.feeder.phonology_Size)(self.layer_Dict['Embedding'])[:2] #RNN. To know details, please see the 'Modules.py'
        self.model = tf.keras.Model(inputs=self.layer_Dict['Input'], outputs= self.layer_Dict['RNN'])   # This line generate a completed model. Training and inference will use this ojbect to run.

        self.model.summary()    # This function display a summary of the model's size.

        self.optimizer = tf.keras.optimizers.Adam(  # Generating an optimizer. In training, model will modify all weights by this object.
            learning_rate= hp_Dict['Train']['Learning_Rate']    # Learning rate. You can set export path at 'Hyper_Parameter.json'
            )   

        self.hidden_Analyzer = Analyzer.Hidden_Analyzer()   # Hidden vector analyizer. To know details, please see the 'Analyzer.py'
        self.output_Analyzer = Analyzer.Ouptut_Analyzer(self.feeder.word_Labels, self.feeder.phoneme_Labels)    # Result analyizer. To know details, please see the 'Analyzer.py'

    @tf.function(   # This decorator helps tensorflow flow control, so speed is up. But, the performance difference is different by the model structure.
        input_signature=[
            tf.TensorSpec(shape=[None, variable_Dict['Max_Word_Length']], dtype=tf.int32),
            tf.TensorSpec(shape=[None, variable_Dict['Max_Pronunciation_Length'], variable_Dict['Phonology_Size']], dtype=tf.float32)
            ],
        autograph= False,
        experimental_relax_shapes= True
        )
    def Train_Step(self, orthographies, pronunciations):    #This function is called by Train function and conducts real training.
        with tf.GradientTape() as tape: #To write, model's gradient.
            _, outputs = self.model(orthographies, training= True)  # Output is generated by model object. The first variable is hidden, so it is ignored in training.
            if not hp_Dict['Phoneme_Feature_File_Path'] is None:    # When output is feature based distribution structure, there are multiple 1 values in target pattern. Thus, loss(error) is calculated by sigmoid based loss function.
                loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
                    labels= pronunciations, # target
                    logits= outputs # output
                    ))
            else:   # When output is one hot, loss(error) is calculated by softmax based loss function.
                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                    labels= pronunciations, # target
                    logits= outputs # output
                    ))
        gradients = tape.gradient(loss, self.model.trainable_variables) # Caculating the gradient of each weight.

        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))  # Updating the weight.

        return loss # To display, loss value is returned.

    @tf.function(   # This decorator helps tensorflow flow control, so speed is up. But, the performance difference is different by the model structure.
        input_signature=[
            tf.TensorSpec(shape=[None, variable_Dict['Max_Word_Length']], dtype=tf.int32)
            ],
        autograph= False,
        experimental_relax_shapes= True
        )
    def Inference_Step(self, orthographies):    #This function is called by Inference function and conducts real inference.
        hiddens, outputs = self.model(orthographies, training= False)   # Hidden and output are generated by model object.
        if not hp_Dict['Phoneme_Feature_File_Path'] is None:    # When output is feature based distribution structure, sigmoid is applied to output.
            outputs = tf.nn.sigmoid(outputs)
        else:   # When output is one hot, softmax is applied to output.
            outputs = tf.nn.softmax(outputs)

        return hiddens, outputs # To calucate result, hidden and output values are returned.
        
    def Restore(self, start_Epoch): # This function is for load previous trained model.
        if start_Epoch == 0:
            return

        checkpoint_File_Path = os.path.join(self.export_Path, 'Checkpoint', 'E_{}.CHECKPOINT.H5'.format(start_Epoch)).replace('\\', '/')    # Setting the weight file name
        
        if not os.path.exists('{}.index'.format(checkpoint_File_Path)): # If there is no weight file, an error will occur.
            raise Exception('There is no checkpoint about epoch \'{}\'.'.format(start_Epoch))

        self.model.load_weights(checkpoint_File_Path)   #Loading
        print('Checkpoint \'{}\' is loaded.'.format(checkpoint_File_Path))

    def Train(self):    # To manage training.
        os.makedirs(os.path.join(self.export_Path, 'Checkpoint'), exist_ok= True)   # Checkpoint save directory is generated if there is no directory.

        current_Epoch = self.feeder.start_Epoch    # Current epoch is used to know checkpoint save and inference timing.
        current_Step_in_Epoch = 0   # This is to display current progress.
        while not self.feeder.is_Finished or len(self.feeder.pattern_Queue) > 0:
            current_Epoch, is_New_Epoch, orthography_Pattern, phonology_Pattern = self.feeder.Get_Pattern() #Getting training patterns. To know details, please see 'Feeder.py' function.

            if is_New_Epoch:
                current_Step_in_Epoch = 0   # Current step is initialized at every epoch starting.
            if is_New_Epoch and current_Epoch % hp_Dict['Train']['Checkpoint_Save_Timing'] == 0:    # When start timing of new epoch, and current epoch is checkpoint saving timing, model save checkpoint.
                self.model.save_weights(os.path.join(self.export_Path, 'Checkpoint', 'E_{}.CHECKPOINT.H5'.format(current_Epoch)).replace('\\', '/'))
                print('Epoch {} checkpoint saved'.format(current_Epoch))
            if is_New_Epoch and current_Epoch % hp_Dict['Train']['Inference_Timing'] == 0:  # When start timing of new epoch, and current epoch is inference timing, model will do inference.
                self.Inference(epoch= current_Epoch, export_Raw= True)
                        
            start_Time = time.time()    # This is to display current progress.
            loss = self.Train_Step( #In this line, model update the weights by patterns. To know details, please see 'Train_Step' function.
                orthographies= orthography_Pattern,
                pronunciations= phonology_Pattern
                )

            display_List = [    # Displaying the trained status.
                'Time: {:0.3f}'.format(time.time() - start_Time),
                'Epoch: {}'.format(current_Epoch),
                'Step in Epoch: {}'.format(current_Step_in_Epoch),
                'Loss: {:0.5f}'.format(loss),
                #'Analyer running...' if any([extract_Thread.is_alive() for extract_Thread in extract_Thread_List]) else ''
                ]
            print('\t\t'.join(display_List))

            current_Step_in_Epoch += 1  # Step update

        # After all training, model will save weights and run last inference.
        self.model.save_weights(os.path.join(self.export_Path, 'Checkpoint', 'E_{}.CHECKPOINT.H5'.format(current_Epoch + 1)).replace('\\', '/'))
        print('Epoch {} checkpoint saved'.format(current_Epoch + 1))
        self.Inference(epoch= current_Epoch + 1, export_Raw= True)

    def Inference(self, epoch= None, letter_String_List= None, added_Pronunciation_Dict = {}, export_Raw= False, file_Tag= 'Inference'):    # To manage inference.
        os.makedirs(os.path.join(self.export_Path, 'Inference'), exist_ok= True)    # Inference save directory is generated if there is no directory.

        if letter_String_List is None:  # If there is no specific inference pattern set, inference uses all training patterns.
            index_Word_Dict = {index: word for word, index in self.feeder.word_Index_Dict.items()}
            letter_String_List = [index_Word_Dict[index] for index in range(len(index_Word_Dict))]

        letter_String_List, pronunciation_List, pattern_List, word_Label_Indices, phoneme_Label_Indices, added_Word_Labels = self.feeder.Get_Inference_Pattern(letter_String_List= letter_String_List, added_Pronunciation_Dict= added_Pronunciation_Dict) #Getting inference patterns. To know details, please see 'Feeder.py' function.

        hiddens_List = []   # A space to store results.
        outputs_List = []   # A space to store results.
        for orthography_Pattern in pattern_List:
            hiddens, outputs = self.Inference_Step( #In this line, model calucate the results. To know details, please see 'Inference_Step' function.
                orthographies= orthography_Pattern
                )
            hiddens_List.append(hiddens)    # Storing results.
            outputs_List.append(outputs)    # Storing results.

        hiddens = np.vstack(hiddens_List)   # Stack all hidden results, and generate a single numpy array.
        outputs = np.vstack(outputs_List)   # Stack all output results, and generate a single numpy array.

        self.Export_Inference(  #In this line, results are analyzed and saved. To know details, please see 'Export_Inference' function.
            letter_String_List,
            pronunciation_List,
            hiddens,
            outputs,
            word_Label_Indices,
            phoneme_Label_Indices,
            added_Word_Labels,
            epoch,
            {key: value for key, value in self.feeder.trained_Pattern_Index_Dict.items() if not epoch is None and key < epoch},
            export_Raw,
            file_Tag
            )

    def Export_Inference(self, letter_String_List, pronunciation_List, hiddens, outputs, word_Label_Indices, phoneme_Label_Indices, added_Word_Labels, epoch, trained_Pattern_Index_Dict, export_Raw= False, file_Tag= 'Inference'):
        #Getting the count how many time each pattern is trained
        trained_Pattern_Count_Dict = {index: 0 for index in self.feeder.word_Index_Dict.values()}   
        for index_List in trained_Pattern_Index_Dict.values():
            for index in index_List:
                trained_Pattern_Count_Dict[index] += 1

        if export_Raw:  #Pickled raw data saving.
            export_Dict = {
                'Epoch': epoch,
                'Pattern_Pair': list(zip(letter_String_List, pronunciation_List)),
                'Hidden': hiddens,
                'Output': outputs,
                'Trained_Pattern_Count_Dict': trained_Pattern_Count_Dict
                }
            with open(os.path.join(self.export_Path, 'Inference', '{}{}.Raw.pickle'.format('E_{}.'.format(epoch) if not epoch is None else '', file_Tag)), 'wb') as f:
                pickle.dump(export_Dict, f, protocol= 4)
        
        pattern_Index_List = list(range(hiddens.shape[0]))  #Getting result index
        pattern_Index_Batch_List = [pattern_Index_List[x:x+hp_Dict['Analyzer']['Batch_Size']] for x in range(0, len(pattern_Index_List), hp_Dict['Analyzer']['Batch_Size'])]    #Analyzing uses much memory, thus pattern must be splitted. Generating batch for analyze. 
        
        hidden_Dict = {}    # A space to store analyzing results.
        result_Dict = {}    # A space to store analyzing results.
        for index, pattern_Index_Batch in enumerate(pattern_Index_Batch_List):
            batch_Hidden_Dict = self.hidden_Analyzer(inputs= hiddens[pattern_Index_Batch])  #In this line, hidden analyzing run. To know details, please see 'Analyzer.py'.
            for key, value in batch_Hidden_Dict.items():
                if not key in hidden_Dict.keys():
                    hidden_Dict[key] = []
                hidden_Dict[key].append(value.numpy())

            batch_Result_Dict = self.output_Analyzer(inputs= outputs[pattern_Index_Batch], word_label_indices= word_Label_Indices[pattern_Index_Batch], phoneme_label_indices= phoneme_Label_Indices[pattern_Index_Batch], added_Word_Labels= added_Word_Labels)    #In this line, result analyzing run. To know details, please see 'Analyzer.py'.
            for key, value in batch_Result_Dict.items():
                if not key in result_Dict.keys():
                    result_Dict[key] = []
                result_Dict[key].append(value.numpy())
                
            progress(index + 1, len(pattern_Index_Batch_List), status='Inference analyzer running')
        print()

        hidden_Dict = {key: np.hstack(value_List) if len(value_List[0].shape)== 1 else np.vstack(value_List) for key, value_List in hidden_Dict.items()}    # Stack each hidden results.
        result_Dict = {key: np.hstack(value_List) if len(value_List[0].shape)== 1 else np.vstack(value_List) for key, value_List in result_Dict.items()}    # Stack each hidden results.

        index_Phoneme_Dict = {index: phoneme for phoneme, index in self.feeder.phoneme_Index_Dict.items() if type(phoneme) == str}  # Result is a index vector. To change to phoneme list, generating index to phoneme matching.

        # Result text file generating
        column_Title_List = [
            'Epoch',
            'Ortho',
            'Phono',
            'Length',
            'Probability',
            'MeanRT',
            'Trained_Count',
            'Cosine_Similarity',
            'Mean_Squared_Error',
            'Euclidean_Distance',
            'Cross_Entropy',
            'Exported_Pronunciation',
            'Accuracy_Max_CS',
            'Accuracy_Min_MSE',
            'Accuracy_Min_ED',
            'Accuracy_Min_CE',
            'Accuracy_Pronunciation',
            'Hidden_Cosine_Similarity',
            'Hidden_Mean_Squared_Error',
            'Hidden_Euclidean_Distance',
            'Hidden_Cross_Entropy',
            ]
        
        export_List = ['\t'.join(column_Title_List)]
        for index, (letter_String, target_Pronunciation, rt_CS, rt_MSE, rt_ED, rt_CE, exported_Pronunciation, acc_CS, acc_MSE, acc_ED, acc_CE, acc_Pronunciation, hidden_CS, hidden_MSE, hidden_ED, hidden_CE) in enumerate(zip(
            letter_String_List,
            pronunciation_List,
            result_Dict['RT', 'CS'],
            result_Dict['RT', 'MSE'],
            result_Dict['RT', 'ED'],
            result_Dict['RT', 'CE'],
            result_Dict['Export', 'Pronunciation'],
            result_Dict['ACC', 'Max_CS'],
            result_Dict['ACC', 'Min_MSE'],
            result_Dict['ACC', 'Min_ED'],
            result_Dict['ACC', 'Min_CE'],
            result_Dict['ACC', 'Pronunciation'],
            hidden_Dict['CS'],
            hidden_Dict['MSE'],
            hidden_Dict['ED'],
            hidden_Dict['CE'],
            )):

            is_Word = letter_String in self.feeder.word_Index_Dict.keys()
            new_Line_List = [
                str(epoch or 'None'),
                letter_String,
                target_Pronunciation,
                str(len(letter_String)),
                str(self.feeder.frequency_Dict[letter_String]) if is_Word else 'None',
                str(self.feeder.human_RT_Dict[letter_String]) if is_Word else 'None',
                str(trained_Pattern_Count_Dict[index]) if is_Word else 'None',
                str(rt_CS),
                str(rt_MSE),
                str(rt_ED),
                str(rt_CE),
                ''.join([index_Phoneme_Dict[phoneme_Index] for phoneme_Index in exported_Pronunciation]),
                str(acc_CS),
                str(acc_MSE),
                str(acc_ED),
                str(acc_CE),
                str(acc_Pronunciation),
                str(hidden_CS),
                str(hidden_MSE),
                str(hidden_ED),
                str(hidden_CE),
                ]
            export_List.append('\t'.join(new_Line_List))

        with open(os.path.join(self.export_Path, 'Inference', '{}{}.Summary.txt'.format('E_{}.'.format(epoch) if not epoch is None else '', file_Tag)), 'w') as f:
            f.write('\n'.join(export_List))

if __name__ == "__main__":
    # To get parameters in terminal.
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-se", "--start_epoch", required=False)
    argParser.add_argument("-e", "--epoch", required=True)
    argParser.add_argument("-idx", "--idx", required=False)
    argument_Dict = vars(argParser.parse_args())

    # Export path is generated.
    extract_Path_List = []
    if hp_Dict['RNN']['Use_Feedback'] and hp_Dict['RNN']['Use_Recurrent']:  #Both of feedback and recurrent connections are used.
        hidden_Calc_Type = 'B'
    if hp_Dict['RNN']['Use_Feedback'] and not hp_Dict['RNN']['Use_Recurrent']:  #Feedback is used and recurrent is NOT used.
        hidden_Calc_Type = 'O'
    if not hp_Dict['RNN']['Use_Feedback'] and hp_Dict['RNN']['Use_Recurrent']:  #recurrent is used and feedback is NOT used.
        hidden_Calc_Type = 'H'
    extract_Path_List.append('HT_{}'.format(hidden_Calc_Type))
    extract_Path_List.append("HU_{}".format(hp_Dict['RNN']['Size']))
    extract_Path_List.append("LR_{}".format(str(hp_Dict['Train']['Learning_Rate'])[2:]))
    extract_Path_List.append("E_{}".format(int(argument_Dict['epoch'])))
    extract_Path_List.append("TT_{}".format(hp_Dict['Train']['Inference_Timing']))
    if hp_Dict['Use_Frequency']:
        extract_Path_List.append("Fre")
    if not hp_Dict['Orthography_Embedding_Size'] is None:
        extract_Path_List.append("EMB_{}".format(hp_Dict['Orthography_Embedding_Size']))
    if not hp_Dict['Phoneme_Feature_File_Path'] is None:
        extract_Path_List.append("DSTR_True")
    if argument_Dict['idx'] is not None:
        extract_Path_List.append("IDX_{}".format(argument_Dict['idx']))
    extract_Path = os.path.join(hp_Dict['Export_Path'], ".".join(extract_Path_List))
    
    new_VOISeR = VOISeR(    #Generating model object
        start_Epoch= int(argument_Dict['start_epoch'] or 0),
        max_Epoch= int(argument_Dict['epoch']),
        export_Path= extract_Path
        )
    new_VOISeR.Restore(start_Epoch= int(argument_Dict['start_epoch'] or 0)) #Load weights.
    new_VOISeR.Train()  #Train model.